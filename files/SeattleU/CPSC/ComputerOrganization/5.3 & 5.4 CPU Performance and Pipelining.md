#ComputerOrganization 

### Performance (CPU Time and Single vs Multiple Cycle Datapaths)

What factors are important when considering performance of a computer system?
- I/O Performance
	- One thing that comes to mind for input/output is GPU (Graphics Processing Unit)
- How many operations per second?
- Memory (how much, access speed, type)
- Power
#### CPU Time Equation
$$CPU \space Time = \frac{seconds}{program} = \frac{instructions}{program}* \frac{avg \space cycles}{instruction}* \frac{seconds}{cycle}$$

$\frac{instructions}{program}$ ← "dynamic" instruction count

$\frac{avg \space cycles}{instruction}$ ← Cycles per instruction (CPI)

$\frac{seconds}{cycle}$ ← Inverse of frequency

#### Misleading / Deceptive statistic Practices
- Only providing clock frequency (often given in GHz).
- Using statistics such as MIPS (Millions of Instructions Per Second) / FLOPS (Floating Point Operations Per Second).
- Selective statistics: Citing only favorable results while omitting others.
- Using biased or irrelevant benchmarks.
- Citing only peak performance numbers while ignoring the average case.
- Vagueness in the use of words like "almost," "nearly," "more," and "less," in comparing performance data.
#### Measuring Performance
Measuring performance is hard!
- Hard to come up with representative and realistic set of benchmarks.
- The compiler and how the program is written factor into the overall time.
- Dependencies on I/O devices, operating systems, libraries, etc.
- Measuring CPU usage at a fine-grained level can be challenging
- Different input sizes can drastically change the results

#### Single Cycle Datapath
- CPI: 1 (every instruction takes 1 cycle)
- Cycle time (inverse of frequency) is set based on time for longest instruction.
- Longest instruction is typically `lw`:
	- Read base address of register
	- Compute effective address using ALU
	- Get value from memory
	- Write value from memory to register file
- Problem: All other instructions waste time.

#### Multiple Cycle Datapath
- Instructions can take multiple cycles to execute.
- Longer instructions take more cycles; shorter instructions take fewer cycles.
- Cycle time is shorter → Less work per cycle
- Is multiple cycle faster than single cycle?

Cycle time is shorter → Improves performance (decreases time)
CPI is higher → Decreases performance (increases time)

##### In class problem
…

### Pipelining

#### What is pipelining?
![[image-25.png|532x319]]
![[image-26.png|531x306]]
- Each instruction does a executes a specific part of its function at once.

#### Pipelined Implementation in ANNA
- Break the execution of the instructions into cycles. 
	- Similar to the multiple cycle datapath.
- Design a separate datapath stage for the execution performed during each cycle.
- Instruction can overlap execution
	- Different instructions can be in different cycles
- Build pipeline registers to communicate between different stages.

#### 5 Stages of ANNA Pipeline
![[image-27.png|586x453]]

**Stage 1: Fetch (IF)**
- Design a datapath that can fetch an instruction form memory every cycle.
	- Use PC to index instruction memory.
	- Increment the PC (assume no branches for now).
- Write everything needed to complete the execution to the pipeline register (*IF/ID*).
	- The next stage will read this pipeline register.
![[image-28.png|537x389]]

**Stage 2: Decode (ID)**
- Reads the IF/ID pipeline register and decodes instruction.
	- Decode is easy: pass opcode to later stages and let them figure out what to do.
- Reads register file.
- Determine immediate (based on instruction).
- Write everything needed to complete execution to pipeline register (*ID/EX*):
	- register values
	- immediate
	- opcode and destination register number (could pass entire instruction)
	- PC+1 (even though decode doesn't use it)
![[image-29.png|598x440]]

**Stage 3: Execute (EX)**
- Performs the proper ALU operation for the instruction specified and the values present in the ID/EX pipeline register.
- Calculates target address in case this is a branch.
	- Also determines if branch is taken or not.
- Write everything needed to complete execution to the pipeline register (*EX/MEM*):
	- ALU result
	- Contents of register src2 (store data)
	- target address (branches)
	- result from branch detection unit
	- instruction bits for opcode and destination register
![[image-30.png|617x448]]

**Stage 4: Memory (MEM)**
- Performs the proper memory operation based on the instruction in EX/MEM pipeline register.
- Also resolves taken branches (more on this later).
- Nothing is done for ALU operations
- Write everything needed to complete execution to the pipeline register (*MEM/WB*)
	- ALU result
	- data from memory (result of load)
	- instruction bits for opcode and destination register
![[image-31.png|585x452]]

**Stage 5: Write back (WB)**
- Completes execution of instruction by writing a value back to the register file.
- Instructions that do not write back to the register file do not do anything.
![[image-32.png|559x407]]


#### Example program

Run the following program on pipeline ANNA processor:
```Assembly
add r1 r2 r3         // PC = 20
lw r4 r5 1
and r6 r2 r3
sw r7 r5 9
or r4 r3 r7
```

**Pipeline Visualization**
![[pipe vis 1.pdf]]

**Time Graph**
![[image-43.png]]

#### Problem: Dependent Instructions
What about:
```Assembly
add r1 r2 r3
sub r4 r5 r1
```
![[unit 5 problem ex.pdf]]
- `r1` is not updated in before sub takes in those registers.

### Data Hazards
**Hazard:** A problem in pipeline that leads to inconsistent results (if unaddressed).
**Data Hazard:** A hazard due to an instruction dependent on data produced by an instruction that is later in the pipeline

First question to ask:
- How far apart, at a minimum, must dependent instructions be to avoid a data hazard?
	- 2 or more instructions away from dependent instructions.
![[image-44.png]]

#### Data Hazards in ANNA
- A *data hazard* occurs in ANNA pipeline when an instruction is dependent on the result of either of the two instructions that precede it.
- Dependent instructions must be separated with at two additional instructions to avoid data hazards.
- The microprocessor detects if a data hazard occurs. If so, stall the pipeline until hazard is cleared.
- Only occurs with registers since writeback (`WB` stage) occurs later than read (`ID` stage).
	- Not a problem with memory since both reads and writes occur in the `MEM` stage.

… fact check `lui r7` part.
![[image-33.png]]
#### Handling Data Hazards
1) Detection:
	- Compare the two register in the decode stage sources (if used) with the destination registers (if written do) of the instructions in the `EX` and `MEM` stage.
2) Stall:
	- Keep current instructions in `IF` and `ID` stages.
	- Pass a `nop` (no operation `add r0 r0 r0`) to `EX` stage.
3) Ending the stall:
	- When dependent instruction reaches `WB` stage, the instructions in `IF` and `ID` stages can move to next stage.
	- Stall will last for 1 or 2 cycles.

**Example**
```Assembly
add r1 r2 r3
and r3 r1 r4
sub r7 r3 r1
lw r7 r3 7
sw r7 r3 6
lui r7 0x56
```

![[Unit 5 stall ex.pdf]]
Time table:
![[image-34.png]]

### Branches in Pipelines and Control Hazards
A **control hazard** occurs anytime a branch or jump instruction is executed.
- `Fetch` stage does not know what address to fetch next instruction from
- In ANNA, branches and `jalr` are resolved in the `MEM` stage.

#### Handling Control Hazards: Stall
1) Detect a branch and stall fetch.
2) Earliest point that a branch can be detected is decode.
	- Detection is easy: look for branch or jump instruction
3) Stall:
	- Keep current instruction in fetch.
	- Pass `nop` to decode stage (not execute)!

**Visual Example**
![[Unit 5 br 1.pdf]]

#### Pipelining Optimization: Branch Prediction
- 3 cycle delay is not necessary all the time.
- Simple prediction: assume branch is not taken.
	- Continue to populate pipeline with instructions at PC+1, PC+2, …
- If branch is not taken → no delay
- If branch is taken:
	- Branch in MEM stage → squash three previous instructions in pipeline
	- Branch in `WB` stage → start fetching from target address in `IF` stage.

**Visual Representation**
![[Unit 5 final vis.pdf]]

**Time Table Example**
![[image-35.png]]

#### Summary For Pipelining in ANNA
- Data hazard stall: Instruction in `ID` stage that is dependent on the instruction in `EX` or `MEM` stage is stalled.
- Control hazard stall: If a branch is taken, the 3 instructions behind it are squashed (stall for 3 cycles)
	- The instruction at the new PC is in the fetch (`IF`) stage when the branch is in writeback (`WB`).
	- No stall if branch is not taken - using "branch not taken" prediction.

##### Class Problem
![[image-36.png]]

#### Pipelining Performance
- CPI (ignore start up)
	- Without hazards: $1$
	- With hazards: $>1$
- Cycle time:
	- Comparable to multiple cycle implementation
- Overall performance:
	- Better than single cycle or multiple cycle
	- Can be improved with enhancements

#### Pipelining in Real Processors
- Some instructions may take longer than one cycle to execute (causing stalls).
	- Examples: Multiply, divide, floating point arithmetic
- Pipelines have more stages.
- Additional stalls due to cache misses.
	- Memory takes longer than one cycle.
- Need to deal with I/O and interrupts.

#### Pipelining as a Computing Concept
- Pipelining is not limited to microprocessors
- Pipelining can be applied to computing systems.
	- Useful in cases where a calculation is broken into phases.
	- Form of parallelism
- Example: Real-time surveillance camera:
	- Camera → Digitizer → Tracker → Alarm

### Additional Performance Optimizations
#### Data Forwarding
- Observation: The value that will be written to the register is know at the `EX` stage for arithmetic and logic operations.
- **Data Forwarding:** Find the result of a previous instruction in a future pipeline register and forward it to the `EX` stage.
- Prevents stalling the pipeline for most (but not all) data hazards.
	- Result for `lw` not know until `MEM` stage.
- Requires more hardware but is worth it!

#### Better Branch Predictions
- Can do more sophisticated predictions:
	- Predict taken (~60% branches are taken)
	- Predict the same as last time
	- Predict based on pattern (loops)
- Modern branch predictors are correct over 90% of the time.

#### Reordering Instructions
In order to avoid data hazards, it may be possible to reorder instructions.
- Goals:
	- Move dependent instructions further away.
	- Cannot violate program functionality
- Especially helpful for slow instructions:
	- loads / store (if not in cache)
	- multiply / divide / FP operations
- Reordering can be done by the compiler or inside the CPU.

#### Parallel Pipelines (Superscalar)
- If you max out the performance of the pipeline and still want more, what can you do?
	- Build a second pipeline
- Two or more instructions can be executed at the same time.
	- Must be independent.
- It is possible to forward data between the two pipelines.
- Lower-level parallelism than multiple cores.

**Intel Core Microarchitecture**
![[image-37.png]]